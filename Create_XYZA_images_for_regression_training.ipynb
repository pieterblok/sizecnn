{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataset of XYZA images that can be trained by the deep learning regression network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train the deep learning regression network, we first need to create a dataset of XYZA images. \n",
    "<br>\n",
    "XYZA images are 4-channel images from which we want to estimate the real-world diameter (with deep learning regression). \n",
    "<br>\n",
    "<br>\n",
    "The XYZA images consist of the following channels:\n",
    "<br>\n",
    "Channel 1: x-coordinates of the visible mask (in millimeters, datatype=np.float32)\n",
    "<br>\n",
    "Channel 2: y-coordinates of the visible mask (in millimeters, datatype=np.float32)\n",
    "<br>\n",
    "Channel 3: z-coordinates of the visible mask (in millimeters, datatype=np.float32)\n",
    "<br>\n",
    "Channel 4: amodal mask (binary image, datatype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to restart your runtime prior to this, to let your installation take effect\n",
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import tifffile\n",
    "from pyexcel_ods import get_data\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# import some miscellaneous libraries\n",
    "from utils import visualize\n",
    "from utils import statistics\n",
    "\n",
    "# import the libraries that are needed for the deep learning regression\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import skimage.transform\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import DatasetCatalog,MetadataCatalog\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "pylab.rcParams['figure.figsize'] = 10,10\n",
    "\n",
    "def imshow(img):\n",
    "    plt.imshow(img[:, :, [2, 1, 0]])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the directories, file locations, etc. (user input is needed here!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = \"./datasets/train_val_test_files\"\n",
    "imgfolder = os.path.join(rootdir, \"orcnn\")\n",
    "xyzimgdir = os.path.join(rootdir, \"regression\")\n",
    "\n",
    "subfolders = [\"train\", \"val\"]\n",
    "store_folder = \"xyza_mask_images\"\n",
    "\n",
    "gtfile = os.path.join(rootdir, \"groundtruth_measurements_broccoli.ods\") ## comment out if there is no ground truth file (also restart the kernel)\n",
    "try:\n",
    "    gt = get_data(gtfile)\n",
    "    gt_file_present = True\n",
    "except:\n",
    "    gt_file_present = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the image inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_orcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (broccoli)\n",
    "\n",
    "cfg.OUTPUT_DIR = \"weights/20201109_broccoli_amodal_visible\"\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_0007999.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model\n",
    "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.01\n",
    "cfg.DATASETS.TRAIN = (\"broccoli_amodal_train\",)\n",
    "cfg.DATASETS.TEST = (\"broccoli_amodal_val\",)\n",
    "\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the biggest dimension from all annotations, such that the created masks will fit into the newly created XYZA image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/10 10:18:51 d2.data.datasets.coco]: \u001b[0mLoaded 1569 images in COCO format from ./datasets/train_val_test_files/orcnn/train/annotations.json\n",
      "\u001b[32m[11/10 10:18:51 d2.data.datasets.coco]: \u001b[0mLoaded 504 images in COCO format from ./datasets/train_val_test_files/orcnn/val/annotations.json\n",
      "\n",
      "Zero-pad size dimension set to: 600\n"
     ]
    }
   ],
   "source": [
    "max_dimension = 0\n",
    "for j in range(len(subfolders)):\n",
    "    subfolder = subfolders[j]\n",
    "\n",
    "    imagefolder = os.path.join(imgfolder, subfolder)\n",
    "    annotationfile = os.path.join(imagefolder, \"annotations.json\")\n",
    "\n",
    "    register_coco_instances(\"broccoli_\" + subfolder, {}, annotationfile, imagefolder)\n",
    "    broccoli_amodal_metadata = MetadataCatalog.get(\"broccoli_\" + subfolder)\n",
    "    dataset_dicts = DatasetCatalog.get(\"broccoli_\" + subfolder)\n",
    "\n",
    "    for i in range(len(dataset_dicts)):\n",
    "        d = dataset_dicts[i]\n",
    "        max_dimensions = []\n",
    "        for k in range(len(d[\"annotations\"])):\n",
    "            max_dimensions.append(max(d[\"annotations\"][k]['bbox'][2], d[\"annotations\"][k]['bbox'][3]))\n",
    "\n",
    "        if np.max(max_dimensions) > max_dimension:\n",
    "            max_dimension = np.max(max_dimensions)\n",
    "            \n",
    "zeropad_dimension = int(statistics.ceil_to_25(max_dimension))\n",
    "print(\"\")\n",
    "print(\"Zero-pad size dimension set to: {0:d}\".format(zeropad_dimension))\n",
    "\n",
    "# Write the zero-pad dimension so it can be used for regression inference as well\n",
    "writedir = os.path.join(rootdir, store_folder)\n",
    "if not os.path.exists(writedir):\n",
    "    os.makedirs(writedir)\n",
    "\n",
    "txtfile = open(os.path.join(writedir,\"zeropad_dimension.txt\"),\"w\")\n",
    "txtfile.write(\"{0:d}\".format(zeropad_dimension))\n",
    "txtfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to find the broccoli head that belongs to the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_object(boxes, boxmethod, coordinates_broccoli_gt, gt_data_present):\n",
    "    distances = []\n",
    "    if np.logical_and(boxes.size > 0, gt_data_present):\n",
    "        for h in range(len(boxes)):\n",
    "            box = boxes[h]\n",
    "            \n",
    "            if boxmethod == \"XYXY\":\n",
    "                x_center = box[0] + ((box[2] - box[0]) / 2)\n",
    "                y_center = box[1] + ((box[3] - box[1]) / 2)\n",
    "            elif boxmethod == \"XYWH\":\n",
    "                x_center = box[0] + (box[2] / 2)\n",
    "                y_center = box[1] + (box[3] / 2)\n",
    "            \n",
    "            distances.append(np.linalg.norm(np.asarray(coordinates_broccoli_gt) - np.asarray((x_center, y_center))))\n",
    "\n",
    "        idx = np.asarray(distances).argmin()\n",
    "    else:\n",
    "        idx = []\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate the overlap area between two rectangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_area(bb1, boxtype1, bb2, boxtype2):  # returns None if rectangles don't intersect\n",
    "    bb1_xmin = bb1[0]\n",
    "    bb1_ymin = bb1[1]\n",
    "    \n",
    "    bb2_xmin = bb2[0]\n",
    "    bb2_ymin = bb2[1]\n",
    "    \n",
    "    if boxtype1 == \"XYWH\":\n",
    "        bb1_xmax = bb1_xmin + bb1[2]\n",
    "        bb1_ymax = bb1_ymin + bb1[3]\n",
    "    elif boxtype1 == \"XYXY\":\n",
    "        bb1_xmax = bb1[2]\n",
    "        bb1_ymax = bb1[3]\n",
    "        \n",
    "    if boxtype2 == \"XYWH\":\n",
    "        bb2_xmax = bb2_xmin + bb2[2]\n",
    "        bb2_ymax = bb2_ymin + bb2[3]\n",
    "    elif boxtype2 == \"XYXY\":\n",
    "        bb2_xmax = bb2[2]\n",
    "        bb2_ymax = bb2[3]\n",
    "              \n",
    "    dx = min(bb1_xmax, bb2_xmax) - max(bb1_xmin, bb2_xmin)\n",
    "    dy = min(bb1_ymax, bb2_ymax) - max(bb1_ymin, bb2_ymin)\n",
    "    if (dx>=0) and (dy>=0):\n",
    "        return dx*dy\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that zero-pads and centers an image to fixed dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeropadding(xyza, dimension=600):\n",
    "    zp = np.zeros((dimension,dimension,xyza.shape[-1])).astype(np.float32)\n",
    "    diffx = int(np.divide(dimension - xyza.shape[0], 2))\n",
    "    diffy = int(np.divide(dimension - xyza.shape[1], 2))\n",
    "    zp[diffx:diffx+xyza.shape[0], diffy:diffy+xyza.shape[1]] = xyza.astype(np.float32)\n",
    "    \n",
    "    return zp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the extreme values of the individual XYZA channels needed for image normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extreme_values(xyza, values):\n",
    "    for h in range(xyza.shape[-1]):\n",
    "        min_value = np.min(zp[:,:,h])\n",
    "        max_value = np.max(zp[:,:,h])\n",
    "\n",
    "        idx1 = (h*2)\n",
    "        idx2 = (h*2)+1\n",
    "\n",
    "        if min_value < values[idx1]:\n",
    "            values[idx1] = min_value\n",
    "            \n",
    "        if max_value > values[idx2]:\n",
    "            values[idx2] = max_value\n",
    "      \n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORCNN image inference that outputs the amodal and visible masks, which are needed to train the deep learning regression network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/10 10:18:51 d2.data.datasets.coco]: \u001b[0mLoaded 1569 images in COCO format from ./datasets/train_val_test_files/orcnn/train/annotations.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1569/1569 [05:55<00:00,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/10 10:24:46 d2.data.datasets.coco]: \u001b[0mLoaded 504 images in COCO format from ./datasets/train_val_test_files/orcnn/val/annotations.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 504/504 [01:55<00:00,  4.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the extreme values for normalization\n",
    "min_x = 0.0\n",
    "max_x = 0.0\n",
    "min_y = 0.0\n",
    "max_y = 0.0\n",
    "min_z = 9999.9\n",
    "max_z = 0.0\n",
    "min_a = 0.0\n",
    "max_a = 0.0\n",
    "values = np.array([min_x, max_x, min_y, max_y, min_z, max_z, min_a, max_a]).astype(np.float32)\n",
    "\n",
    "if gt_file_present:\n",
    "    for j in range(len(subfolders)):\n",
    "        subfolder = subfolders[j]\n",
    "        \n",
    "        writedir = os.path.join(rootdir, store_folder, subfolder)\n",
    "\n",
    "        if not os.path.exists(writedir):\n",
    "            os.makedirs(writedir)\n",
    "\n",
    "        imagefolder = os.path.join(imgfolder, subfolder)\n",
    "        annotationfile = os.path.join(imagefolder, \"annotations.json\")\n",
    "        \n",
    "        # Register the amodal dataset\n",
    "        register_coco_instances(\"broccoli_amodal_\" + subfolder, {}, annotationfile, imagefolder)\n",
    "        broccoli_amodal_metadata = MetadataCatalog.get(\"broccoli_amodal_\" + subfolder)\n",
    "        dataset_dicts = DatasetCatalog.get(\"broccoli_amodal_\" + subfolder)\n",
    "        \n",
    "\n",
    "        for i in tqdm(range(len(dataset_dicts))):\n",
    "            # Load the RGB image\n",
    "            imgname = dataset_dicts[i][\"file_name\"]\n",
    "            basename = os.path.basename(imgname)\n",
    "            img = cv2.imread(imgname)\n",
    "\n",
    "\n",
    "            # Load the XYZ image\n",
    "            xyzimgname = basename.replace(\"rgb\", \"xyz\")\n",
    "            xyzimgname = xyzimgname.replace(\".png\", \".tiff\")\n",
    "            xyzimg = tifffile.imread(os.path.join(xyzimgdir, subfolder, xyzimgname))\n",
    "            zimg = np.expand_dims(xyzimg[:,:,-1], axis=2)\n",
    "\n",
    "\n",
    "            # Do the image inference and extract the outputs from Mask R-CNN\n",
    "            start_time = time.time()\n",
    "            outputs = predictor(img)\n",
    "            instances = outputs[\"instances\"].to(\"cpu\")\n",
    "            classes = instances.pred_classes.numpy()\n",
    "            scores = instances.scores.numpy()\n",
    "            boxes = instances.pred_boxes.tensor.numpy()\n",
    "\n",
    "            # Procedure to check whether we are dealing with ORCNN or MRCNN\n",
    "            if \"pred_visible_masks\" in instances._fields:\n",
    "                amodal_masks = instances.pred_masks.numpy()\n",
    "                visible_masks = instances.pred_visible_masks.numpy()\n",
    "            else:\n",
    "                visible_masks = instances.pred_masks.numpy()\n",
    "\n",
    "\n",
    "            # Get the ground truth data\n",
    "            real_diameter = 0\n",
    "            gt_data_present = False\n",
    "\n",
    "            for k in range(1, len(gt['groundtruth_measurements_broccoli'])):\n",
    "                gt_data = gt['groundtruth_measurements_broccoli'][k]\n",
    "                if gt_data:\n",
    "                    if gt_data[0] == basename:\n",
    "                        gt_data_present = True\n",
    "                        plant_id = gt_data[1]\n",
    "                        real_diameter = gt_data[2]\n",
    "                        x_center_gt = gt_data[3]\n",
    "                        y_center_gt = gt_data[4]\n",
    "                        coordinates_broccoli_gt = (x_center_gt, y_center_gt)\n",
    "\n",
    "\n",
    "            # Procedure to load the annotations in case there is no detection \n",
    "            d = dataset_dicts[i]\n",
    "            classes_annot = []\n",
    "            boxes_annot = []\n",
    "            amodal_masks_poly = []\n",
    "            visible_masks_poly = []\n",
    "            for k in range(len(d[\"annotations\"])):\n",
    "                classes_annot.append(d[\"annotations\"][k]['category_id'])\n",
    "                boxes_annot.append(d[\"annotations\"][k]['bbox'])\n",
    "                amodal_masks_poly.append(d[\"annotations\"][k]['segmentation'])\n",
    "                visible_masks_poly.append(d[\"annotations\"][k]['visible_mask'])\n",
    "\n",
    "\n",
    "            # Get the annotations    \n",
    "            amodal_masks_annot = visualize.make_mask_img(amodal_masks_poly, d['height'], d['width'], \"polylines\")\n",
    "            visible_masks_annot = visualize.make_mask_img(visible_masks_poly, d['height'], d['width'], \"polylines\")\n",
    "\n",
    "\n",
    "            # Get the bounding box of the broccoli annotation that belongs to the size estimation of the ground truth\n",
    "            idx = find_closest_object(np.asarray(boxes_annot), \"XYWH\", coordinates_broccoli_gt, gt_data_present)\n",
    "            bbox_annot = np.asarray(boxes_annot)[idx]\n",
    "            class_annot = np.asarray(classes_annot)[idx]\n",
    "\n",
    "\n",
    "            # Extract the intersection areas between the annotation box and the detected boxes\n",
    "            if boxes.size > 0:\n",
    "                int_areas = np.zeros(len(boxes)).astype(np.float32)\n",
    "                for h in range(len(boxes)):\n",
    "                    box = boxes[h]\n",
    "                    cur_class = classes[h]\n",
    "                    if cur_class == class_annot:\n",
    "                        int_areas[h] = overlap_area(box, \"XYXY\", bbox_annot, \"XYWH\")\n",
    "            else:\n",
    "                int_areas = np.zeros(1).astype(np.float32)\n",
    "\n",
    "\n",
    "            # Get the masks of the detected broccoli head that belongs to the ground truth data\n",
    "            if np.count_nonzero(int_areas) > 0:\n",
    "                idx = int_areas.argmax()\n",
    "                bbox = boxes[idx]\n",
    "                amodal_mask = np.expand_dims(amodal_masks[idx], axis=2)\n",
    "                visible_mask = np.expand_dims(visible_masks[idx], axis=2)\n",
    "\n",
    "            # If there is not a detection that belongs to the ground truth data, use the annotation\n",
    "            else:\n",
    "                bbox = [bbox_annot[0], bbox_annot[1], bbox_annot[0]+bbox_annot[2], bbox_annot[1]+bbox_annot[3]]\n",
    "                amodal_mask = np.expand_dims(amodal_masks_annot[idx], axis=2)\n",
    "                visible_mask = np.expand_dims(visible_masks_annot[idx], axis=2)\n",
    "\n",
    "\n",
    "            # Get the XYZ data of the visible mask\n",
    "            xyzimg_clip = xyzimg[int(bbox[1]):int(bbox[3]),int(bbox[0]):int(bbox[2]),:]\n",
    "            visible_mask_clip = visible_mask[int(bbox[1]):int(bbox[3]),int(bbox[0]):int(bbox[2]),:]\n",
    "            xyz_mask = np.multiply(xyzimg_clip, visible_mask_clip)\n",
    "\n",
    "\n",
    "            # Make a 4-channel image with the XYZ data of the visible mask and the binary amodal mask\n",
    "            xyza = np.zeros((xyz_mask.shape[0], xyz_mask.shape[1], 4), dtype=np.float32) \n",
    "            xyza[:,:,:3] = xyz_mask.astype(np.float32)\n",
    "            amodal_mask_clip = amodal_mask[int(bbox[1]):int(bbox[3]),int(bbox[0]):int(bbox[2]),:]\n",
    "            xyza[:,:,3] = amodal_mask_clip.reshape(xyz_mask.shape[0], xyz_mask.shape[1]).astype(np.float32)\n",
    "\n",
    "\n",
    "            # Apply zeropadding to resize the final mask to a fixed size\n",
    "            zp = zeropadding(xyza, zeropad_dimension)\n",
    "            \n",
    "            \n",
    "            # Extract the extreme values needed for normalization (when training and infering the regression network)\n",
    "            values = get_extreme_values(zp, values)\n",
    "            \n",
    "            \n",
    "            # Write the final xyza image and its label\n",
    "            tifffile.imsave(os.path.join(writedir, xyzimgname), zp)\n",
    "            \n",
    "            \n",
    "            # Write the label\n",
    "            txt_name = xyzimgname.replace(\".tiff\", \".txt\")\n",
    "            txtfile = open(os.path.join(writedir,txt_name),\"w\")\n",
    "            txtfile.write(\"{0:.1f}\".format(real_diameter))\n",
    "            txtfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the normalization values so it can be used for regression training and inference\n",
    "writedir = os.path.join(rootdir, store_folder)\n",
    "if not os.path.exists(writedir):\n",
    "    os.makedirs(writedir)\n",
    "\n",
    "txtfile = open(os.path.join(writedir,\"normalization_values.txt\"),\"w\")\n",
    "for value in values:\n",
    "    txtfile.write(\"{0:.2f}\".format(value))\n",
    "    txtfile.write(\"\\r\\n\")\n",
    "txtfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
